{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Salting to handle Data Skew when writing Data sets.\n",
    "\n",
    "\n",
    "In this case, my Skew Column is 'bucket' which has records per value varying from 100 to 10000 columns. Writing this data without resolving the skew:\n",
    "\n",
    "- Makes writes slower\n",
    "- Put pressure on a few Executors\n",
    "\n",
    "Steps to resolve the Skew:\n",
    "\n",
    "1. Add a Salt Column.\n",
    "2. Repartition by Salt Column.\n",
    "3. Write Partitioned by Skew Column.\n",
    "\n",
    "\n",
    "## Read the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T22:34:57.713106Z",
     "start_time": "2021-03-16T22:34:55.460023Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|    bucket|prefix|\n",
      "+----------+------+\n",
      "|5UTV1TLVXT|  5296|\n",
      "|5UTV1TLVXT|  7400|\n",
      "|5UTV1TLVXT|  8573|\n",
      "|5UTV1TLVXT|  4216|\n",
      "|5UTV1TLVXT|  9965|\n",
      "+----------+------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "input_df = spark.read.csv(\"s3://<bucket>/dataskew/files/\")\n",
    "input_df = input_df.toDF(\"bucket\",\"prefix\")\n",
    "input_df.createOrReplaceTempView(\"input_df_v\")\n",
    "input_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T22:35:52.078252Z",
     "start_time": "2021-03-16T22:35:49.822878Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n",
      "|    bucket|group_counts|\n",
      "+----------+------------+\n",
      "|GZVQF7CQRP|        1000|\n",
      "|5UTV1TLVXT|       10000|\n",
      "|TMG607P21Z|         100|\n",
      "+----------+------------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"Select bucket, count(1) as group_counts from input_df_v group by bucket\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Split Size and Salt Key column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T22:35:10.297574Z",
     "start_time": "2021-03-16T22:35:08.041603Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "group_counts=spark.sql(\"Select bucket, count(1) as group_counts from input_df_v group by bucket\").rdd.collectAsMap()\n",
    "group_counts_b = sc.broadcast(group_counts) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T22:35:18.469661Z",
     "start_time": "2021-03-16T22:35:16.215846Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------------+\n",
      "|    bucket|prefix|  salted_key|\n",
      "+----------+------+------------+\n",
      "|5UTV1TLVXT|  5296|5UTV1TLVXT30|\n",
      "|5UTV1TLVXT|  7400|5UTV1TLVXT56|\n",
      "|5UTV1TLVXT|  8573|5UTV1TLVXT43|\n",
      "|5UTV1TLVXT|  4216|5UTV1TLVXT32|\n",
      "|5UTV1TLVXT|  9965|5UTV1TLVXT19|\n",
      "+----------+------+------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf\n",
    "import random\n",
    "import math\n",
    "\n",
    "# partition size = 100\n",
    "salt_udf = udf(lambda key: key+str(random.randint(1,math.ceil(group_counts_b.value[key]/100))), StringType())\n",
    "\n",
    "input_df=input_df.withColumn(\"salted_key\", salt_udf(input_df.bucket))\n",
    "input_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T22:35:26.150828Z",
     "start_time": "2021-03-16T22:35:26.123088Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_df.createOrReplaceTempView(\"input_df_v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T22:36:05.894976Z",
     "start_time": "2021-03-16T22:36:02.625956Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n",
      "|    bucket|group_counts|\n",
      "+----------+------------+\n",
      "|GZVQF7CQRP|          10|\n",
      "|5UTV1TLVXT|         100|\n",
      "|TMG607P21Z|           1|\n",
      "+----------+------------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"Select bucket, count(distinct salted_key) as group_counts from input_df_v group by bucket\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T22:40:10.130679Z",
     "start_time": "2021-03-16T22:40:08.882417Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+\n",
      "|  salted_key|group_counts|\n",
      "+------------+------------+\n",
      "|5UTV1TLVXT94|          79|\n",
      "|5UTV1TLVXT63|          80|\n",
      "+------------+------------+\n",
      "only showing top 2 rows"
     ]
    }
   ],
   "source": [
    "spark.sql(\"Select salted_key, count(1) as group_counts from input_df_v group by salted_key order by 2 asc\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T22:55:18.479905Z",
     "start_time": "2021-03-16T22:55:07.192533Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+\n",
      "|  salted_key|group_counts|\n",
      "+------------+------------+\n",
      "|5UTV1TLVXT55|         128|\n",
      "|5UTV1TLVXT45|         124|\n",
      "+------------+------------+\n",
      "only showing top 2 rows"
     ]
    }
   ],
   "source": [
    "spark.sql(\"Select salted_key, count(1) as group_counts from input_df_v group by salted_key order by 2 desc\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the record counts for each salted key varies a bit from 128 to 79 but that should not be a problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T22:40:18.262931Z",
     "start_time": "2021-03-16T22:40:17.523662Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29"
     ]
    }
   ],
   "source": [
    "partition_columns=[\"salted_key\"]\n",
    "input_df=input_df.repartition(*partition_columns)\n",
    "input_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T22:48:32.861553Z",
     "start_time": "2021-03-16T22:48:23.574912Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------------+\n",
      "|    bucket|prefix|  salted_key|\n",
      "+----------+------+------------+\n",
      "|5UTV1TLVXT|  6781|5UTV1TLVXT67|\n",
      "|5UTV1TLVXT|  1899|5UTV1TLVXT67|\n",
      "|5UTV1TLVXT|  2992|5UTV1TLVXT67|\n",
      "+----------+------+------------+\n",
      "only showing top 3 rows"
     ]
    }
   ],
   "source": [
    "input_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T23:00:30.144086Z",
     "start_time": "2021-03-16T23:00:14.844674Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "partition_columns='bucket'\n",
    "input_df.write.mode(\"OVERWRITE\").partitionBy(partition_columns).csv(\"s3://<bucket>/dataskew/output/\",header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T22:49:40.954879Z",
     "start_time": "2021-03-16T22:49:38.700236Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- prefix: string (nullable = true)\n",
      " |-- salted_key: string (nullable = true)\n",
      " |-- bucket: string (nullable = true)\n",
      " |-- filename: string (nullable = false)"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import input_file_name \n",
    "\n",
    "output_df=spark.read.csv(\"s3://<bucket>/dataskew/output/\",header=True)\n",
    "output_df=output_df.withColumn(\"filename\",input_file_name())\n",
    "output_df.createOrReplaceTempView(\"output_df_v\")\n",
    "output_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T22:50:54.354669Z",
     "start_time": "2021-03-16T22:50:52.100911Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|    bucket|files|\n",
      "+----------+-----+\n",
      "|5UTV1TLVXT|   28|\n",
      "|GZVQF7CQRP|    8|\n",
      "|TMG607P21Z|    1|\n",
      "+----------+-----+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT bucket, count(distinct filename) as files from output_df_v group by bucket ORDER by 2 DESC\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
