{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Launch EMR 6.2.0 with application JupyterEnterpriseGateway 2.1.0 selected.\n",
    "\n",
    "### Step 2: Ensure hdfs:///user/emr-notebook has user privileges for user emr-notebook:\n",
    "\n",
    "```\n",
    "$> hdfs dfs -mkdir /user/emr-notebook\n",
    "$> hdfs dfs -chown emr-notebook:emr-notebook /user/emr-notebook\n",
    "```\n",
    "\n",
    "### Step 3: Install findspark on the cluster.\n",
    "\n",
    "```\n",
    "$> sudo pip3 install findspark\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 124 ms, sys: 25.1 ms, total: 149 ms\n",
      "Wall time: 12.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## 1st cell of notebook needs to intialize Spark session.\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "config_list = [\n",
    "    ('spark.sql.execution.arrow.pyspark.enabled','true'),\n",
    "    ('spark.sql.execution.arrow.pyspark.fallback.enabled','false'),\n",
    "    ('spark.pyspark.virtualenv.enabled','true'),\n",
    "    ('spark.pyspark.virtualenv.type','native'),\n",
    "    ('spark.pyspark.virtualenv.bin.path','/usr/bin/virtualenv')    \n",
    "]\n",
    "\n",
    "conf = pyspark.SparkConf().setAll(config_list)\n",
    "\n",
    "spark = SparkSession.builder.master('yarn')\\\n",
    "    .config(conf=conf)\\\n",
    "    .appName('myapp')\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 205 µs, sys: 33 µs, total: 238 µs\n",
      "Wall time: 256 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'3.0.1-amzn-0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 67.6 ms, sys: 23.2 ms, total: 90.8 ms\n",
      "Wall time: 123 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import random\n",
    "\n",
    "\n",
    "pdf = pd.DataFrame(np.random.randint(0,100,size=(1000000, 4)), columns=list('ABCD'))\n",
    "df = spark.createDataFrame(pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
