{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Hash based Partitions on a S3 dataset.\n",
    "\n",
    "Partition Pruning can significantly speed up queries on an S3 dataset.\n",
    "For datasets where a distinct partition field cannot be identified, and especially for ranged get use cases e.g. query by user id or query by order id etc. partitioning the dataset by hashing the key field into a fixed number of partitions can signficantly speed up queries.\n",
    "\n",
    "In this notebook we are using the Spark crc32 function to hash a field and then modulo to divide the hash into n buckets.\n",
    "\n",
    "Let's look a sample orders dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>17</td><td>application_1608318781561_0019</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-45-148.us-west-2.compute.internal:20888/proxy/application_1608318781561_0019/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-45-148.us-west-2.compute.internal:8042/node/containerlogs/container_1608318781561_0019_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df=spark.read.parquet(\"s3://<databucket>/xyzzypq/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------------------+-------+-----------+-------------------+--------+----------+--------+---------+-------+------------------+------------------+----------+\n",
      "|          DISCOUNT|LAST_MODIFIED_TIMESTAMP|LINE_ID|LINE_NUMBER|         ORDER_DATE|ORDER_ID|PRODUCT_ID|QUANTITY|SHIP_MODE|SITE_ID|       SUPPLY_COST|               TAX|UNIT_PRICE|\n",
      "+------------------+-----------------------+-------+-----------+-------------------+--------+----------+--------+---------+-------+------------------+------------------+----------+\n",
      "|505.15211456522854|    2013-09-20T22:40:04|      1|          1|2013-09-20T00:00:00|  115855|       410|      35|  ONE-DAY|    386|389.05923561572445| 625.2594318622095|     618.0|\n",
      "|189.40738665133208|    2013-09-20T22:40:04|      2|          2|2013-09-20T00:00:00|  115855|        28|      76|  ONE-DAY|    386| 289.2103998597069|  298.229409855354|     498.0|\n",
      "|160.04322240073517|    2013-09-20T22:40:04|      3|          3|2013-09-20T00:00:00|  115855|       899|      57|  ONE-DAY|    386|203.81834973681305|172.28458770681308|     737.0|\n",
      "+------------------+-----------------------+-------+-----------+-------------------+--------+----------+--------+---------+-------+------------------+------------------+----------+\n",
      "only showing top 3 rows"
     ]
    }
   ],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add a PARTITION_KEY using the crc32 hash function and divide into 20 buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------------------+-------+-----------+-------------------+--------+----------+--------+---------+-------+------------------+------------------+----------+-------------+\n",
      "|          DISCOUNT|LAST_MODIFIED_TIMESTAMP|LINE_ID|LINE_NUMBER|         ORDER_DATE|ORDER_ID|PRODUCT_ID|QUANTITY|SHIP_MODE|SITE_ID|       SUPPLY_COST|               TAX|UNIT_PRICE|PARTITION_KEY|\n",
      "+------------------+-----------------------+-------+-----------+-------------------+--------+----------+--------+---------+-------+------------------+------------------+----------+-------------+\n",
      "|505.15211456522854|    2013-09-20T22:40:04|      1|          1|2013-09-20T00:00:00|  115855|       410|      35|  ONE-DAY|    386|389.05923561572445| 625.2594318622095|     618.0|           19|\n",
      "|189.40738665133208|    2013-09-20T22:40:04|      2|          2|2013-09-20T00:00:00|  115855|        28|      76|  ONE-DAY|    386| 289.2103998597069|  298.229409855354|     498.0|           19|\n",
      "|160.04322240073517|    2013-09-20T22:40:04|      3|          3|2013-09-20T00:00:00|  115855|       899|      57|  ONE-DAY|    386|203.81834973681305|172.28458770681308|     737.0|           19|\n",
      "+------------------+-----------------------+-------+-----------+-------------------+--------+----------+--------+---------+-------+------------------+------------------+----------+-------------+\n",
      "only showing top 3 rows"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "df=df.withColumn(\"PARTITION_KEY\",expr(\"mod(crc32(concat(ORDER_ID)),20)\"))\n",
    "df.createOrReplaceTempView(\"order_lines_v\")\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+\n",
      "|PARTITION_KEY|RECORD_COUNT|\n",
      "+-------------+------------+\n",
      "|            0|      171761|\n",
      "|            1|      172459|\n",
      "|            2|      173135|\n",
      "|            3|      171889|\n",
      "|            4|      171900|\n",
      "|            5|      171185|\n",
      "|            6|      172586|\n",
      "|            7|      172765|\n",
      "|            8|      172008|\n",
      "|            9|      172573|\n",
      "|           10|      172168|\n",
      "|           11|      170935|\n",
      "|           12|      173715|\n",
      "|           13|      172674|\n",
      "|           14|      173131|\n",
      "|           15|      173176|\n",
      "|           16|      174665|\n",
      "|           17|      173062|\n",
      "|           18|      172022|\n",
      "|           19|      172939|\n",
      "+-------------+------------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"Select PARTITION_KEY, count(1) as RECORD_COUNT FROM  order_lines_v GROUP BY PARTITION_KEY ORDER BY 1\").show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the dataset is divided uniformly into 20 buckets, let's write it the data parititioned by the partition column.\n",
    "We are also sorting the data within each partition by ORDER_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "partitionColumns=[\"PARTITION_KEY\"]\n",
    "df.repartition(*partitionColumns).sortWithinPartitions(\"ORDER_ID\").write.mode(\"OVERWRITE\").partitionBy(partitionColumns).parquet(\"s3://<databucket>/xyzzypq-p1/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying the dataset using Amazon Athena:\n",
    "\n",
    "Amazon Athena also supports the crc32 function so we can derive the partition from the ORDER_ID field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "\n",
    "import boto3,time\n",
    "import pandas as pd\n",
    "\n",
    "region='us-east-1'\n",
    "defaultdb=\"tempdb\"\n",
    "default_output=\"s3://<athena_query_results_bucket>/\"\n",
    "\n",
    "## execute Athena SQL\n",
    "def executeQuery(query, database=defaultdb, s3_output=default_output, poll=0.5):\n",
    "    athena = boto3.client('athena',region_name=region)\n",
    "    response = athena.start_query_execution(\n",
    "        QueryString=query,\n",
    "        QueryExecutionContext={\n",
    "            'Database': database\n",
    "            },\n",
    "        ResultConfiguration={\n",
    "            'OutputLocation': s3_output,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    print('Execution ID: ' + response['QueryExecutionId'])\n",
    "    queryExecutionId=response['QueryExecutionId']\n",
    "    state='QUEUED'\n",
    "    while( state=='RUNNING' or state=='QUEUED'):\n",
    "       response = athena.get_query_execution(QueryExecutionId=queryExecutionId)\n",
    "       state=response['QueryExecution']['Status']['State']\n",
    "       print (state)\n",
    "       if  state=='RUNNING' or state=='QUEUED':\n",
    "            time.sleep(poll)\n",
    "       elif (state=='FAILED'):\n",
    "             print (response['QueryExecution']['Status']['StateChangeReason'])\n",
    "        \n",
    "        \n",
    "    #print (response)    \n",
    "    return response\n",
    "\n",
    "## Read from Athena to a Pandas Dataframe\n",
    "def read_from_athena(sql):\n",
    "    response=executeQuery(sql)\n",
    "    return pd.read_csv(response['QueryExecution']['ResultConfiguration']['OutputLocation'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution ID: 8a1419d3-b644-497f-8525-5b1f24325163\n",
      "QUEUED\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "SUCCEEDED\n",
      "CPU times: user 178 ms, sys: 8.01 ms, total: 186 ms\n",
      "Wall time: 5.33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "sql=\"\"\"SELECT * FROM \"tempdb\".\"xyzzypq\" where order_id = 100\"\"\"\n",
    "\n",
    "response=executeQuery(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution ID: 82b76930-09b4-47dc-be15-0b79bba9e125\n",
      "QUEUED\n",
      "RUNNING\n",
      "RUNNING\n",
      "SUCCEEDED\n",
      "CPU times: user 39.7 ms, sys: 1.07 ms, total: 40.8 ms\n",
      "Wall time: 2.27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "sql=\"\"\"SELECT * FROM \n",
    "(Select * from \"tempdb\".\"xyzzypq_p1\" \n",
    "where partition_key=cast(mod(crc32(to_utf8(cast(order_id as varchar))),20) as varchar))\n",
    "where order_id = 100\"\"\"\n",
    "\n",
    "response=executeQuery(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "**We see an improvement of around 50% in query response times for this query.** The subquery can be easily saved to an Amazon Athena view so that users do not have to specify the partition_key in their queries but it is automatically applied in the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
